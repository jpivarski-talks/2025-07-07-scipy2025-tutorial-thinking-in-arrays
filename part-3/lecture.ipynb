{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882b9184",
   "metadata": {},
   "source": [
    "# Part 3: JIT-compilation with Numba and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6fdc85",
   "metadata": {},
   "source": [
    "In lecture 2 you've seen that fusing operations is powerful and crucial for performance!\n",
    "\n",
    "Essentially, we're able to get rid of _intermediate_ arrays by \"fusing operations\" using just-in-time (JIT) compilation by applying these operations in a _single_ iteration over our data.\n",
    "\n",
    "Fusing operations is a tricky task however. There are a few ways to achieve this for array processing in Python, and I'd like to highlight two of them:\n",
    "\n",
    "- Numba: https://numba.pydata.org\n",
    "- JAX: https://github.com/jax-ml/jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9914c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Numba\n",
    "import numba as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a37b3",
   "metadata": {},
   "source": [
    "Let's consider the quadratic formula example again, and compare the runtimes for NumPy, Numba, and JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6227aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data\n",
    "a = np.random.uniform(5, 10, 5_000_000)\n",
    "b = np.random.uniform(10, 20, 5_000_000)\n",
    "c = np.random.uniform(-0.1, 0.1, 5_000_000)\n",
    "\n",
    "\n",
    "# Setup quadratic formula\n",
    "def quadratic_formula(a, b, c):\n",
    "    return (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e56fd",
   "metadata": {},
   "source": [
    "NumPy case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab610b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r3\n",
    "\n",
    "quadratic_formula(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d0563",
   "metadata": {},
   "source": [
    "Numba case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdebf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit  # JIT compile!\n",
    "def quadratic_formula_numba(a, b, c):\n",
    "    n = a.shape[0]\n",
    "    out = np.empty(n)\n",
    "    for i in range(n):\n",
    "        out[i] = (-b[i] + np.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1\n",
    "\n",
    "quadratic_formula_numba(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10 -r3\n",
    "\n",
    "quadratic_formula_numba(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b672a8",
   "metadata": {},
   "source": [
    "JAX case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data\n",
    "a_jax = jnp.asarray(a)\n",
    "b_jax = jnp.asarray(b)\n",
    "c_jax = jnp.asarray(c)\n",
    "\n",
    "\n",
    "@jax.jit  # JIT compile!\n",
    "def quadratic_formula_jax(a, b, c):\n",
    "    return (-b + jnp.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1\n",
    "\n",
    "quadratic_formula_jax(a_jax, b_jax, c_jax).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410130fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10 -r3\n",
    "\n",
    "quadratic_formula_jax(a_jax, b_jax, c_jax).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d047444",
   "metadata": {},
   "source": [
    "The first invocation for JAX & Numba took longer than consecutive ones. That's the compile time! Afterwards the compiled function is cached...\n",
    "\n",
    "But JAX is still much faster, why?\n",
    "\n",
    "One important difference is that JAX uses as many threads as it has access to. Numba is single-threaded, but can be multithreaded using `parallel=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(parallel=True)  # JIT compile with `parallel=True`!\n",
    "def quadratic_formula_numba_parallel(a, b, c):\n",
    "    n = a.shape[0]\n",
    "    out = np.empty(n)\n",
    "    for i in nb.prange(n):  # note: `range` -> `nb.prange`\n",
    "        out[i] = (-b[i] + np.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1\n",
    "\n",
    "quadratic_formula_numba_parallel(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10 -r3\n",
    "\n",
    "quadratic_formula_numba_parallel(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a60ef",
   "metadata": {},
   "source": [
    "Now we're roughly on-par with JAX and Numba with ~2-3ms runtime compared to NumPy's ~23ms.\n",
    "\n",
    "\n",
    "You might have noticed a fundamental difference between JAX and Numba in how those kernels are written: \n",
    "\n",
    "- Numba forces[<sup id=\"fn1-back\">1</sup>](#fn1) you to write _imperative_ code\n",
    "- JAX forces[<sup id=\"fn2-back\">2</sup>](#fn2) you to write _array-oriented_ code\n",
    "\n",
    "\n",
    "![image](https://raw.githubusercontent.com/jpivarski-talks/2023-12-18-hsf-india-tutorial-bhubaneswar/refs/heads/main/img/slow-fast-imperative-vectorized.svg)\n",
    "\n",
    "\n",
    "\n",
    "[<sup id=\"fn1\">1</sup>](#fn1-back) <sup>Can be written array-oriented with `nb.vectorize`.</sup> \n",
    "\n",
    "[<sup id=\"fn2\">2</sup>](#fn2-back) <sup>Can be written imperative with JAX's own loop primitives, e.g. `jax.lax.scan`.</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9ef0f",
   "metadata": {},
   "source": [
    "### Limitations of Numba\n",
    "\n",
    "You can not JIT-compile arbitrary Python functions. Numba can only JIT-compile a subset of Python, i.e. everything that's \"known\" to Numba as a type (mostly NumPy & NumPy operations).\n",
    "\n",
    "For more information, see: https://numba.readthedocs.io/en/stable/user/5minguide.html#will-numba-work-for-my-code.\n",
    "\n",
    "\n",
    "Check the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def sum_dict_values(d):\n",
    "    out = 0.\n",
    "    for v in d.values():\n",
    "        out += v\n",
    "    return out\n",
    "\n",
    "sum_dict_values({\"a\": 1.0, \"b\": 2.0, \"c\": 3.0})  # Fails, because `dict` is not a known type for Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ff828",
   "metadata": {},
   "source": [
    "### Limitations of JAX\n",
    "\n",
    "JAX infers the operations that are going to be run through a \"tracing step\". Essentially, JAX will run your program once with shallow array objects (no data, just metadata). That let's you JIT-compile all of Python, **but** you can't JIT-compile data-dependent operations.\n",
    "\n",
    "For more \"sharp bits\", see: https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html.\n",
    "\n",
    "Check the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-dependent operations are not traceable\n",
    "\n",
    "@jax.jit\n",
    "def accumulate_if(arr):\n",
    "    print(arr)\n",
    "    if jnp.any(arr > 3):\n",
    "        return jnp.sum(arr)\n",
    "    else:\n",
    "        return jnp.prod(arr)\n",
    "\n",
    "\n",
    "array = jnp.array([1., 2., 3., 4., 5.])\n",
    "print(accumulate_if(array))  # Fails, because jnp.any(arr > 3) is not traceable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be44fdc",
   "metadata": {},
   "source": [
    "Another limitation of JAX is that you can't JIT compile programs with unknown shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4cac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sum_greater_than_three(arr):\n",
    "    return jnp.sum(arr[arr > 3.0])\n",
    "\n",
    "\n",
    "array = jnp.array([1., 2., 3., 4., 5.])\n",
    "print(sum_greater_than_three(array))  # Fails, because the output shape of `arr[arr > 3.0]` is not inferrable through tracing (without data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec1619",
   "metadata": {},
   "source": [
    "### Impure functions are dangerous with JIT compilation! (Numba & JAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d80a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_sum = False\n",
    "\n",
    "@nb.njit\n",
    "def accumulate(arr):\n",
    "    if do_sum:\n",
    "        return np.sum(arr)\n",
    "    else:\n",
    "        return np.prod(arr)\n",
    "\n",
    "\n",
    "array = np.array([1., 2., 3., 4., 5.])\n",
    "print(\"Accumulate with `np.prod`:\", accumulate(array))\n",
    "\n",
    "# now we switch `do_sum` on!\n",
    "do_sum = True\n",
    "print(\"Accumulate with `np.sum`:\", accumulate(array), f\"...Hey, this should've been {np.sum(array)} instead!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9718f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_sum = False\n",
    "\n",
    "@jax.jit\n",
    "def accumulate(arr):\n",
    "    if do_sum:\n",
    "        return jnp.sum(arr)\n",
    "    else:\n",
    "        return jnp.prod(arr)\n",
    "\n",
    "\n",
    "array = jnp.array([1., 2., 3., 4., 5.])\n",
    "\n",
    "print(\"Accumulate with `jnp.prod`:\", accumulate(array))\n",
    "\n",
    "# now we switch `do_sum` on!\n",
    "do_sum = True\n",
    "print(\"Accumulate with `jnp.sum`:\", accumulate(array), f\"...Hey, this should've been {jnp.sum(array)} instead!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee86c88",
   "metadata": {},
   "source": [
    "We can see why in the JAX case: that's because the traced program _never knew_ that there's a `sum` operation in the first place _and_ the compiled function is cached based on their input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Traced program:\\n\", accumulate.trace(array).jaxpr)\n",
    "print()\n",
    "print(\"HLO program:\\n\", accumulate.lower(array).as_text()) # this is the program that get's compiled by XLA compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cd834",
   "metadata": {},
   "source": [
    "### JIT-compilation for GPUs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afa1a0",
   "metadata": {},
   "source": [
    "\n",
    "#### Numba.cuda\n",
    "\n",
    "Numba exposes CUDA to Python through the `numba.cuda` module. Here, the programming model follows very closely the CUDA C language by NVidia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform square matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f5595",
   "metadata": {},
   "source": [
    "#### JAX on GPUs\n",
    "\n",
    "JAX can run on GPUs without any code modifications (the power of array-oriented programming). The _symbolic_ operations of the IR (JaxPr) will just dispatch to GPU kernels instead of CPU kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs on CPU and GPU, depending on the available `jax.devices()`\n",
    "\n",
    "@jax.jit\n",
    "def matmul(A, B): # -> C\n",
    "    \"\"\"\n",
    "    Perform square matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    return A @ B\n",
    "\n",
    "\n",
    "print(\"Available devices:\", jax.devices())\n",
    "\n",
    "# explicitely move data to devic (CPU or GPU):\n",
    "array = jax.device_put(jnp.arange(10), device=jax.devices()[0])\n",
    "\n",
    "print(f\"{array=} lives on\", array.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8a09f",
   "metadata": {},
   "source": [
    "On to the [project.ipynb](project.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
